# -*- coding: utf-8 -*-
"""langchain-sample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bem2eVVly8FrA6xBBlpwCGnpAdR7AR7b
"""

!pip install langchain openai

import os

# os.environ["OPENAI_API_KEY"] = "secret-api-key"
os.environ["OPENAI_API_KEY"] = "sk-"

from langchain.llms import OpenAI

llm = OpenAI(
    model_name="text-davinci-003",
    temperature=0.0,
)

result = llm("自己紹介求！")
print(result)

from langchain.chat_models import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage

# LangChain で Chat Completions API を使う。
chat = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.0,
)

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="俺はジョンだ！"),
    AIMessage(content="こんにちはジョンさん、どのように手伝えますかあ？"),
    HumanMessage(content="私の名前がわかるかな、ははは"),
]

result = chat(messages)
print(result)
print(result.content)

from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

# 標準出力にストリーム出力する。
chat = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.0,
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
)

messages = [HumanMessage(content="自己紹介求む！")]
result = chat(messages)

from langchain.prompts import PromptTemplate

template = """
以下の料理のレシピを考えてください。

料理名: {dish}
"""

# プログラムで文字列の一部を置き換えているだけで、
# 内部で LLM を呼び出すことはしていない。
prompt = PromptTemplate(
    input_variables=["dish"],
    template=template,
)

result = prompt.format(
    dish = "カレーー",
)

print(result)

from langchain.prompts import (
    ChatPromptTemplate,
    PromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import HumanMessage, SystemMessage

# ChatPromptTemplate は PromptTemplate を Chat Completions API の形式に対応させたもの。
chat_prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template("あなたは{country}料理のプロフェッショナルです。"),
    HumanMessagePromptTemplate.from_template("以下の料理のレシピを考えてください。\n\n料理名: {dish}"),
])

messages = chat_prompt.format_prompt(
    country="中国",
    dish="肉じゃが",
).to_messages()

print(messages)

from pydantic import BaseModel, Field

class Recipe(BaseModel):
    ingredients: list[str] = Field(description="ingredients of the dish")
    steps: list[str] = Field(description="steps to make the dish")

from langchain.output_parsers import PydanticOutputParser

parser = PydanticOutputParser(pydantic_object=Recipe)

format_instructions = parser.get_format_instructions()

print(format_instructions)

template = """
料理のレシピを考えてください！！

{format_instructions}

料理名: {dish}
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["dish"],
    partial_variables={"format_instructions": format_instructions}
)

formatted_prompt = prompt.format(
    dish = "カレーー",
)

print(formatted_prompt)

# PromptTemplate と Output parser を合わせる。
chat = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.0,
)
messages = [HumanMessage(content=formatted_prompt)]
output = chat(messages)
print(output.content)

# Pydantic に変換して使う。
recipe = parser.parse(output.content)
print(type(recipe))
print(recipe)

"""# LLMChain

Prompt Template, Language model, Output Parser を繋ぐ

"""

from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
from pydantic import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser

class Recipe(BaseModel):
    ingredients: list[str] = Field(description="ingredients of the dish")
    steps: list[str] = Field(description="steps to make the dish")

# Output Parser
output_parser = PydanticOutputParser(pydantic_object=Recipe)

template = """
料理のレシピを考えてください！！

{format_instructions}

料理名: {dish}
"""

# Prompt Template
prompt = PromptTemplate(
    template=template,
    input_variables=["dish"],
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
)

# Language model
chat = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.0,
)

from langchain import LLMChain

chain = LLMChain(
    prompt = prompt,
    llm = chat,
    output_parser = output_parser,
)

recipe = chain.run(
    dish = "カレーーー",
)

print(type(recipe))
print(recipe)

"""## SimpleSequentialChain の例！"""

chat = ChatOpenAI(
    model_name = "gpt-3.5-turbo",
    temperature=0.0,
)

cot_template = """
以下の質問に回答してください。

質問: {question}

ステップバイステップで考えましょう。
"""

cot_prompt = PromptTemplate(
    input_variables=["question"],
    template = cot_template,
)

# 1st chain:
# Zero-shot CoT でステップバイステップで考えさせる。
cot_chain = LLMChain(
    llm = chat,
    prompt = cot_prompt,
)

summarize_template = """
以下の文章を結論だけ一言に要約してください。

{input}
"""

summarize_prompt = PromptTemplate(
    input_variables = ["input"],
    template = summarize_template,
)

# 2st chain:
# 出力内容を要約する。
summarize_chain = LLMChain(
    llm = chat,
    prompt = summarize_prompt,
)

# 2つを繋げた Chain を作成する。

from langchain.chains import SimpleSequentialChain

cot_summarize_chain = SimpleSequentialChain(
    chains = [cot_chain, summarize_chain],
)

result = cot_summarize_chain(
    "スーパーで10個のリンゴを買いました。友達に2つ、妹に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？",
)

print(result["output"])

"""# Memory"""

from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory

chat = ChatOpenAI(
    model_name = "gpt-4",
    temperature = 0.0,
)

conversation = ConversationChain(
    llm = chat,
    memory = ConversationBufferMemory(),
)

while True:
    user_message = input("You: ")
    ai_message = conversation.run(input=user_message)
    print(f"AI: {ai_message}")

# Chat Completions API へのリクエストの内容をログ出力させたい！
import logging
logging.getLogger('openai').setLevel(logging.DEBUG)
