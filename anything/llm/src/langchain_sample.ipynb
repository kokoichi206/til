{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53xt4TNJDGB7",
        "outputId": "61a12bb2-409e-426a-ed78-3b2c9dc279a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.331-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain)\n",
            "  Downloading langsmith-0.0.57-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, openai, dataclasses-json, langchain\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.331 langsmith-0.0.57 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.28.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"secret-api-key\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-\""
      ],
      "metadata": {
        "id": "61aqu6WwD04B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(\n",
        "    model_name=\"text-davinci-003\",\n",
        "    temperature=0.0,\n",
        ")\n",
        "\n",
        "result = llm(\"自己紹介求！\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHNGbh6dDaEf",
        "outputId": "c6c2533a-8a00-4441-88ea-4220ac40f6da"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "はじめまして！\n",
            "私は、Taroと申します。\n",
            "大学では、経済学を専攻しています。\n",
            "趣味は、スポーツ観戦や旅行などです。\n",
            "特に、サッカーが大好きで、日本代表の試合を観戦するのが大好きです。\n",
            "今後ともよろしくお願いします！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# LangChain で Chat Completions API を使う。\n",
        "chat = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0.0,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"俺はジョンだ！\"),\n",
        "    AIMessage(content=\"こんにちはジョンさん、どのように手伝えますかあ？\"),\n",
        "    HumanMessage(content=\"私の名前がわかるかな、ははは\"),\n",
        "]\n",
        "\n",
        "result = chat(messages)\n",
        "print(result)\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F85pN4PhEG2r",
        "outputId": "8603bee0-cd3e-48d3-9584-32eec99163cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='はい、先ほど「俺はジョンだ！」とおっしゃいましたので、お名前はジョンさんですね。どのようなお手伝いができますか？'\n",
            "はい、先ほど「俺はジョンだ！」とおっしゃいましたので、お名前はジョンさんですね。どのようなお手伝いができますか？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# 標準出力にストリーム出力する。\n",
        "chat = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0.0,\n",
        "    streaming=True,\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],\n",
        ")\n",
        "\n",
        "messages = [HumanMessage(content=\"自己紹介求む！\")]\n",
        "result = chat(messages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRbseJYfFGCk",
        "outputId": "385b2779-6c23-43c1-ea6d-2363082a9184"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "こんにちは！私はAIのチャットボットです。私の名前はOpenAI Assistantです。私は様々なトピックについて質問に答えたり、会話をすることができます。私は自然言語処理技術を使用しており、ユーザーの質問や要求に対して最善の回答を提供するように設計されています。どのようにお手伝いできますか？"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "以下の料理のレシピを考えてください。\n",
        "\n",
        "料理名: {dish}\n",
        "\"\"\"\n",
        "\n",
        "# プログラムで文字列の一部を置き換えているだけで、\n",
        "# 内部で LLM を呼び出すことはしていない。\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"dish\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "result = prompt.format(\n",
        "    dish = \"カレーー\",\n",
        ")\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqCAZGuaGafQ",
        "outputId": "31685131-3ba4-4c43-d4d1-cc0b43b707f5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "以下の料理のレシピを考えてください。\n",
            "\n",
            "料理名: カレーー\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# ChatPromptTemplate は PromptTemplate を Chat Completions API の形式に対応させたもの。\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"あなたは{country}料理のプロフェッショナルです。\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"以下の料理のレシピを考えてください。\\n\\n料理名: {dish}\"),\n",
        "])\n",
        "\n",
        "messages = chat_prompt.format_prompt(\n",
        "    country=\"中国\",\n",
        "    dish=\"肉じゃが\",\n",
        ").to_messages()\n",
        "\n",
        "print(messages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ww_I-OmHBJJ",
        "outputId": "b9090878-3b8a-4f39-a458-d593144e90e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='あなたは中国料理のプロフェッショナルです。'), HumanMessage(content='以下の料理のレシピを考えてください。\\n\\n料理名: 肉じゃが')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "    steps: list[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Recipe)\n",
        "\n",
        "format_instructions = parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsRl6VHsIrY7",
        "outputId": "f5d6ac85-255e-433e-de5f-64facb9006a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"ingredients\": {\"title\": \"Ingredients\", \"description\": \"ingredients of the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"steps\": {\"title\": \"Steps\", \"description\": \"steps to make the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"ingredients\", \"steps\"]}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "料理のレシピを考えてください！！\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "料理名: {dish}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"dish\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "formatted_prompt = prompt.format(\n",
        "    dish = \"カレーー\",\n",
        ")\n",
        "\n",
        "print(formatted_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtuzAZxTJWrq",
        "outputId": "7217d112-b428-4557-d7b5-fc0528914271"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "料理のレシピを考えてください！！\n",
            "\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"ingredients\": {\"title\": \"Ingredients\", \"description\": \"ingredients of the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"steps\": {\"title\": \"Steps\", \"description\": \"steps to make the dish\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"ingredients\", \"steps\"]}\n",
            "```\n",
            "\n",
            "料理名: カレーー\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PromptTemplate と Output parser を合わせる。\n",
        "chat = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0.0,\n",
        ")\n",
        "messages = [HumanMessage(content=formatted_prompt)]\n",
        "output = chat(messages)\n",
        "print(output.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61I4JxQuJ6MJ",
        "outputId": "839c2b54-fa4d-4a90-f4f4-9e79cc222b28"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"ingredients\": [\n",
            "    \"玉ねぎ\",\n",
            "    \"にんじん\",\n",
            "    \"じゃがいも\",\n",
            "    \"牛肉\",\n",
            "    \"カレールー\",\n",
            "    \"水\"\n",
            "  ],\n",
            "  \"steps\": [\n",
            "    \"玉ねぎ、にんじん、じゃがいもを切ります。\",\n",
            "    \"牛肉を炒めます。\",\n",
            "    \"野菜を加えて炒めます。\",\n",
            "    \"水を加えて煮込みます。\",\n",
            "    \"カレールーを加えて溶かします。\",\n",
            "    \"煮込んで完成です。\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pydantic に変換して使う。\n",
        "recipe = parser.parse(output.content)\n",
        "print(type(recipe))\n",
        "print(recipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9S2byErKaaY",
        "outputId": "6e421a32-efc1-4fea-edee-880d261b7e97"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.Recipe'>\n",
            "ingredients=['玉ねぎ', 'にんじん', 'じゃがいも', '牛肉', 'カレールー', '水'] steps=['玉ねぎ、にんじん、じゃがいもを切ります。', '牛肉を炒めます。', '野菜を加えて炒めます。', '水を加えて煮込みます。', 'カレールーを加えて溶かします。', '煮込んで完成です。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMChain\n",
        "\n",
        "Prompt Template, Language model, Output Parser を繋ぐ\n"
      ],
      "metadata": {
        "id": "jau_3Je8MV7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
        "    steps: list[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "# Output Parser\n",
        "output_parser = PydanticOutputParser(pydantic_object=Recipe)\n",
        "\n",
        "template = \"\"\"\n",
        "料理のレシピを考えてください！！\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "料理名: {dish}\n",
        "\"\"\"\n",
        "\n",
        "# Prompt Template\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"dish\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# Language model\n",
        "chat = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0.0,\n",
        ")\n"
      ],
      "metadata": {
        "id": "NnxW3aPZL2H8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "chain = LLMChain(\n",
        "    prompt = prompt,\n",
        "    llm = chat,\n",
        "    output_parser = output_parser,\n",
        ")\n",
        "\n",
        "recipe = chain.run(\n",
        "    dish = \"カレーーー\",\n",
        ")\n",
        "\n",
        "print(type(recipe))\n",
        "print(recipe)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is4Mb41iL16g",
        "outputId": "d1310a7e-694e-4a57-f99d-af732aaceeda"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.Recipe'>\n",
            "ingredients=['玉ねぎ', 'にんじん', 'じゃがいも', '牛肉', 'カレールー', '水'] steps=['玉ねぎ、にんじん、じゃがいもを切ります。', '牛肉を炒めます。', '野菜を加えて炒めます。', '水を加えて煮込みます。', 'カレールーを加えて溶かします。', '煮込んで完成です。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SimpleSequentialChain の例！"
      ],
      "metadata": {
        "id": "399Un4xUPEXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(\n",
        "    model_name = \"gpt-3.5-turbo\",\n",
        "    temperature=0.0,\n",
        ")\n",
        "\n",
        "cot_template = \"\"\"\n",
        "以下の質問に回答してください。\n",
        "\n",
        "質問: {question}\n",
        "\n",
        "ステップバイステップで考えましょう。\n",
        "\"\"\"\n",
        "\n",
        "cot_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template = cot_template,\n",
        ")\n",
        "\n",
        "# 1st chain:\n",
        "# Zero-shot CoT でステップバイステップで考えさせる。\n",
        "cot_chain = LLMChain(\n",
        "    llm = chat,\n",
        "    prompt = cot_prompt,\n",
        ")\n",
        "\n",
        "summarize_template = \"\"\"\n",
        "以下の文章を結論だけ一言に要約してください。\n",
        "\n",
        "{input}\n",
        "\"\"\"\n",
        "\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables = [\"input\"],\n",
        "    template = summarize_template,\n",
        ")\n",
        "\n",
        "# 2st chain:\n",
        "# 出力内容を要約する。\n",
        "summarize_chain = LLMChain(\n",
        "    llm = chat,\n",
        "    prompt = summarize_prompt,\n",
        ")\n",
        "\n",
        "# 2つを繋げた Chain を作成する。\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "cot_summarize_chain = SimpleSequentialChain(\n",
        "    chains = [cot_chain, summarize_chain],\n",
        ")\n",
        "\n",
        "result = cot_summarize_chain(\n",
        "    \"スーパーで10個のリンゴを買いました。友達に2つ、妹に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\",\n",
        ")\n",
        "\n",
        "print(result[\"output\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJUA8GQDOHOH",
        "outputId": "688706d5-f9d7-4584-b50c-52c8988bbbea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "残りのリンゴは10個です。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory"
      ],
      "metadata": {
        "id": "bgAi75qGU5md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    model_name = \"gpt-4\",\n",
        "    temperature = 0.0,\n",
        ")\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm = chat,\n",
        "    memory = ConversationBufferMemory(),\n",
        ")\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    ai_message = conversation.run(input=user_message)\n",
        "    print(f\"AI: {ai_message}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "zZU_F_5LU4aY",
        "outputId": "dc244741-e097-44bf-82f9-bc12fced4117"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: pien\n",
            "AI: It seems like you've typed \"pien,\" which doesn't form a clear question or statement in English. If it's a typo, could you please correct it? If it's a term in another language, could you please provide more context or specify the language? I'm here to help!\n",
            "You: ぴえん\n",
            "AI: Ah, you're using Japanese! \"ぴえん\" is a slang term in Japanese that's often used on the internet. It's an onomatopoeic word that represents a crying sound, similar to \"boo hoo\" in English. It's often used to express sadness or disappointment.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-48b5d6fe414a>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mai_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"AI: {ai_message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat Completions API へのリクエストの内容をログ出力させたい！\n",
        "import logging\n",
        "logging.getLogger('openai').setLevel(logging.DEBUG)\n"
      ],
      "metadata": {
        "id": "ueEIHZ8LV7Fs"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}